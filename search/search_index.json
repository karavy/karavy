{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"karavy-demo/","text":"Karavy Demo \u2013 Creazione di un Cluster Tenant \u00b6 Requisiti \u00b6 Per questa demo sono necessarie due macchine (fisiche o virtuali): Prima VM : Cluster di management Seconda VM : Worker node del cluster tenant Devono essere sulla stessa rete (o comunque raggiungibili tra loro). In questa demo sono state usate VM su KVM. 1. Installazione del Cluster di Management \u00b6 1.1 Sistema Operativo \u00b6 Installare Ubuntu 24.04 Server e assegnare un IP statico ad entrambe le macchine. Durante l'installazione attivare anche il server ssh 1.2 Installazione di k0s \u00b6 Collegarsi alla vm che ospiter\u00e0 il cluster di management e installare k0s curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh sudo k0s install controller --single sudo k0s start 1.3 Installare e configurare kubectl per l'accesso al cluster di management \u00b6 sudo -i snap install kubectl --classic mkdir /home/<user>/.kube k0s kubeconfig admin > /home/<user>/.kube/config chown -R <user>:<group> /home/<user>/.kube exit 1.4 Installazione di Helm \u00b6 curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null sudo apt-get install apt-transport-https --yes echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm 1.5 Installazione di storage condiviso per i volumi di etcd \u00b6 2. Installazione Componenti del Cluster \u00b6 2.1 Cert-Manager \u00b6 helm repo add jetstack https://charts.jetstack.io --force-update helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.17.2 --set crds.enabled = true 2.2 MetalLB \u00b6 kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.9/config/manifests/metallb-native.yaml Esempio configurazione IPAddressPool e L2Advertisement: apiVersion : metallb.io/v1beta1 kind : IPAddressPool metadata : name : karavy-demo-pool namespace : metallb-system spec : addresses : - 192.168.1.240-192.168.1.250 --- apiVersion : metallb.io/v1beta1 kind : L2Advertisement metadata : name : karavy-demo-advertisement namespace : metallb-system 3. Installazione Karavy-Core \u00b6 git clone https://github.com/karavy/karavy-demo.git kubectl apply -k karavy-demo/core Verificare che l\u2019operatore sia avviato nel namespace karavy-core . 4. Creazione del Tenant \u00b6 4.1 Namespace e CRD \u00b6 kubectl create ns tenant-1 Applicare il manifest (verificare IP e CIDR per evitare sovrapposizioni): apiVersion : tenants.karavy.io/v1 kind : K8sTenant metadata : labels : tenant-name : tenant-1 name : tenant-1 namespace : tenant-1 spec : kubeMasterDomain : cluster.local tenantApiServer : additionalArgs : - --v=10 antiAffinity : true certificateDurationHours : 8760 keycloak : enabled : false replicas : 1 tenantControllerManager : antiAffinity : true certificateDurationHours : 8760 replicas : 1 tenantDNSServer : certificateDurationHours : 8760 tenantEtcdServer : additionalArgs : - --initial-advertise-peer-urls=$(URI_SCHEME)://$(HOSTNAME).$(SERVICE_NAME).$(K8S_NAMESPACE):2380 antiAffinity : true certificateDurationHours : 8760 replicas : 3 storageClassName : nfs-csi storageSize : 10Gi tenantNetwork : cniApplication : type : calico version : v3.29.2 KubeDefaultSvc : 10.240.0.1 podCidr : 16 PodNetwork : 10.120.0.0 serviceCidr : 16 serviceNetwork : 10.240.0.0 tenantKubeDNSIP : 10.240.0.10 tenantKubeDomain : tenant-1.local tenantScheduler : antiAffinity : true certificateDurationHours : 8760 replicas : 1 tenantVersions : tenantCrioVersion : v1.31.5 tenantEtcdVersion : v3.5.18 tenantKubeVersion : v1.31.5 usersCertificates : certificateDurationHours : 8760 winNodeSupport : true workerCertificates : certificateDurationHours : 8760 5. Connessione al Cluster Tenant \u00b6 kubectl get secret -n tenant-1 admin-conf -o jsonpath = '{.data.tenant-1-config}' | base64 -d > /tmp/kubeconfig export KUBECONFIG = /tmp/kubeconfig 6. Installazione Karavy-Worker \u00b6 kubectl apply -k karavy-demo/worker 7. Configurazione Worker Node \u00b6 7.1 Accesso SSH \u00b6 Generare una chiave SSH e creare un secret: kubectl create secret generic sshkey --from-file = id_rsa = ~/.ssh/id_rsa --namespace = tenant-1 7.2 Manifest Worker \u00b6 apiVersion : tenants.karavy.io/v1 kind : K8sTenantWorker metadata : name : tenant-1-worker-01 namespace : tenant-1 spec : tenantName : tenant-1 hostname : worker-01 hostIP : 172.25.58.100 sshKeySecret : sshkey operatingSystem : ubuntu2404 sshPort : 22 8. Installazione Karavy-Net \u00b6 kubectl apply -k karavy-demo/net 9. Definizione Rotte \u00b6 apiVersion : net.karavy.io/v1 kind : KaravyRouting metadata : labels : app.kubernetes.io/name : karavy-net app.kubernetes.io/managed-by : kustomize name : karavy-tenant-1-routing namespace : karavy-net spec : tenantName : tenant-1 mainServiceCidr : 10.96.0.0/12 mainPodCidr : 10.244.0.0/16 mainCNIType : userdefined mainClusterNodes : - mainClusterNodeType : controlplane name : kavary-main-cluster servicePriority : 10 tenantServiceCidr : 10.240.0.0/16 tenantPodCidr : 10.120.0.0/16 tenantCNIType : calico tenantClusterNodes : - ip : 192.168.3.216 name : worker-01 servicePriority : 10 sshKey : sshkey sshPort : 22","title":"Karavy Demo"},{"location":"karavy-demo/#karavy-demo-creazione-di-un-cluster-tenant","text":"","title":"Karavy Demo \u2013 Creazione di un Cluster Tenant"},{"location":"karavy-demo/#requisiti","text":"Per questa demo sono necessarie due macchine (fisiche o virtuali): Prima VM : Cluster di management Seconda VM : Worker node del cluster tenant Devono essere sulla stessa rete (o comunque raggiungibili tra loro). In questa demo sono state usate VM su KVM.","title":"Requisiti"},{"location":"karavy-demo/#1-installazione-del-cluster-di-management","text":"","title":"1. Installazione del Cluster di Management"},{"location":"karavy-demo/#11-sistema-operativo","text":"Installare Ubuntu 24.04 Server e assegnare un IP statico ad entrambe le macchine. Durante l'installazione attivare anche il server ssh","title":"1.1 Sistema Operativo"},{"location":"karavy-demo/#12-installazione-di-k0s","text":"Collegarsi alla vm che ospiter\u00e0 il cluster di management e installare k0s curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh sudo k0s install controller --single sudo k0s start","title":"1.2 Installazione di k0s"},{"location":"karavy-demo/#13-installare-e-configurare-kubectl-per-laccesso-al-cluster-di-management","text":"sudo -i snap install kubectl --classic mkdir /home/<user>/.kube k0s kubeconfig admin > /home/<user>/.kube/config chown -R <user>:<group> /home/<user>/.kube exit","title":"1.3 Installare e configurare kubectl per l'accesso al cluster di management"},{"location":"karavy-demo/#14-installazione-di-helm","text":"curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null sudo apt-get install apt-transport-https --yes echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm","title":"1.4 Installazione di Helm"},{"location":"karavy-demo/#15-installazione-di-storage-condiviso-per-i-volumi-di-etcd","text":"","title":"1.5 Installazione di storage condiviso per i volumi di etcd"},{"location":"karavy-demo/#2-installazione-componenti-del-cluster","text":"","title":"2. Installazione Componenti del Cluster"},{"location":"karavy-demo/#21-cert-manager","text":"helm repo add jetstack https://charts.jetstack.io --force-update helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.17.2 --set crds.enabled = true","title":"2.1 Cert-Manager"},{"location":"karavy-demo/#22-metallb","text":"kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.9/config/manifests/metallb-native.yaml Esempio configurazione IPAddressPool e L2Advertisement: apiVersion : metallb.io/v1beta1 kind : IPAddressPool metadata : name : karavy-demo-pool namespace : metallb-system spec : addresses : - 192.168.1.240-192.168.1.250 --- apiVersion : metallb.io/v1beta1 kind : L2Advertisement metadata : name : karavy-demo-advertisement namespace : metallb-system","title":"2.2 MetalLB"},{"location":"karavy-demo/#3-installazione-karavy-core","text":"git clone https://github.com/karavy/karavy-demo.git kubectl apply -k karavy-demo/core Verificare che l\u2019operatore sia avviato nel namespace karavy-core .","title":"3. Installazione Karavy-Core"},{"location":"karavy-demo/#4-creazione-del-tenant","text":"","title":"4. Creazione del Tenant"},{"location":"karavy-demo/#41-namespace-e-crd","text":"kubectl create ns tenant-1 Applicare il manifest (verificare IP e CIDR per evitare sovrapposizioni): apiVersion : tenants.karavy.io/v1 kind : K8sTenant metadata : labels : tenant-name : tenant-1 name : tenant-1 namespace : tenant-1 spec : kubeMasterDomain : cluster.local tenantApiServer : additionalArgs : - --v=10 antiAffinity : true certificateDurationHours : 8760 keycloak : enabled : false replicas : 1 tenantControllerManager : antiAffinity : true certificateDurationHours : 8760 replicas : 1 tenantDNSServer : certificateDurationHours : 8760 tenantEtcdServer : additionalArgs : - --initial-advertise-peer-urls=$(URI_SCHEME)://$(HOSTNAME).$(SERVICE_NAME).$(K8S_NAMESPACE):2380 antiAffinity : true certificateDurationHours : 8760 replicas : 3 storageClassName : nfs-csi storageSize : 10Gi tenantNetwork : cniApplication : type : calico version : v3.29.2 KubeDefaultSvc : 10.240.0.1 podCidr : 16 PodNetwork : 10.120.0.0 serviceCidr : 16 serviceNetwork : 10.240.0.0 tenantKubeDNSIP : 10.240.0.10 tenantKubeDomain : tenant-1.local tenantScheduler : antiAffinity : true certificateDurationHours : 8760 replicas : 1 tenantVersions : tenantCrioVersion : v1.31.5 tenantEtcdVersion : v3.5.18 tenantKubeVersion : v1.31.5 usersCertificates : certificateDurationHours : 8760 winNodeSupport : true workerCertificates : certificateDurationHours : 8760","title":"4.1 Namespace e CRD"},{"location":"karavy-demo/#5-connessione-al-cluster-tenant","text":"kubectl get secret -n tenant-1 admin-conf -o jsonpath = '{.data.tenant-1-config}' | base64 -d > /tmp/kubeconfig export KUBECONFIG = /tmp/kubeconfig","title":"5. Connessione al Cluster Tenant"},{"location":"karavy-demo/#6-installazione-karavy-worker","text":"kubectl apply -k karavy-demo/worker","title":"6. Installazione Karavy-Worker"},{"location":"karavy-demo/#7-configurazione-worker-node","text":"","title":"7. Configurazione Worker Node"},{"location":"karavy-demo/#71-accesso-ssh","text":"Generare una chiave SSH e creare un secret: kubectl create secret generic sshkey --from-file = id_rsa = ~/.ssh/id_rsa --namespace = tenant-1","title":"7.1 Accesso SSH"},{"location":"karavy-demo/#72-manifest-worker","text":"apiVersion : tenants.karavy.io/v1 kind : K8sTenantWorker metadata : name : tenant-1-worker-01 namespace : tenant-1 spec : tenantName : tenant-1 hostname : worker-01 hostIP : 172.25.58.100 sshKeySecret : sshkey operatingSystem : ubuntu2404 sshPort : 22","title":"7.2 Manifest Worker"},{"location":"karavy-demo/#8-installazione-karavy-net","text":"kubectl apply -k karavy-demo/net","title":"8. Installazione Karavy-Net"},{"location":"karavy-demo/#9-definizione-rotte","text":"apiVersion : net.karavy.io/v1 kind : KaravyRouting metadata : labels : app.kubernetes.io/name : karavy-net app.kubernetes.io/managed-by : kustomize name : karavy-tenant-1-routing namespace : karavy-net spec : tenantName : tenant-1 mainServiceCidr : 10.96.0.0/12 mainPodCidr : 10.244.0.0/16 mainCNIType : userdefined mainClusterNodes : - mainClusterNodeType : controlplane name : kavary-main-cluster servicePriority : 10 tenantServiceCidr : 10.240.0.0/16 tenantPodCidr : 10.120.0.0/16 tenantCNIType : calico tenantClusterNodes : - ip : 192.168.3.216 name : worker-01 servicePriority : 10 sshKey : sshkey sshPort : 22","title":"9. Definizione Rotte"}]}