{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"karavy-demo/","text":"Karavy Demo \u2013 Creazione di un Cluster Tenant \u00b6 Requisiti \u00b6 Per questa demo sono necessarie due macchine (fisiche o virtuali): Prima VM : Cluster di management CPU: 2 RAM: 4GB Disk: 40GB Seconda VM : Worker node del cluster tenant CPU: 2 RAM: 4GB Disk: 40GB Terza VM : (Opzionale) Installazione del server NFS per i volumi etcd del cluster tenant . Nel caso in cui si disponga gi\u00e0 di storage condiviso disponibile per il cluster di management, si pu\u00f2 utilizzare quello. CPU: 1 RAM: 2GB Disk: 40GB Le macchine devono essere sulla stessa rete (o comunque raggiungibili tra loro senza limitazione di porte o protocolli). In questa demo sono state usate VM su KVM. 1. Installazione del Cluster di Management \u00b6 1.1 Sistema Operativo \u00b6 Installare Ubuntu 24.04 Server e assegnare un IP statico ad entrambe le macchine. Durante l'installazione attivare anche il server ssh 1.2 Installazione di k0s \u00b6 Collegarsi alla vm che ospiter\u00e0 il cluster di management e installare k0s curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh sudo k0s install controller --single sudo k0s start 1.3 Installare e configurare kubectl per l'accesso al cluster di management \u00b6 sudo -i snap install kubectl --classic mkdir /home/<user>/.kube k0s kubeconfig admin > /home/<user>/.kube/config chown -R <user>:<group> /home/<user>/.kube exit 1.4 Installazione di Helm \u00b6 curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null sudo apt-get install apt-transport-https --yes echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm 1.5 Installazione di storage condiviso per i volumi di etcd \u00b6 1.5.1 Installazione e configurazione server NFS \u00b6 Installare il server NFS e creare la directory che conterr\u00e0 i volumi kubernetes sudo apt install nfs-server sudo mkdir /home/nfsdata Creare la condivisione NFS sudo -i echo '/home/nfsdata <cluster_network>(no_root_squash,rw,sync,no_subtree_check)' >> /etc/exports exportfs -va 2. Installazione Componenti del Cluster \u00b6 2.1 Cert-Manager \u00b6 helm repo add jetstack https://charts.jetstack.io --force-update helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.17.2 --set crds.enabled = true 2.2 MetalLB \u00b6 kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.9/config/manifests/metallb-native.yaml 2.2.1 Configurazione IPAddressPool e L2Advertisement \u00b6 Configurare Metallb. Nei manifest seguenti si utilizza la modalit\u00e0 layer2 apiVersion : metallb.io/v1beta1 kind : IPAddressPool metadata : name : karavy-demo-pool namespace : metallb-system spec : addresses : - <first_pool_ip>-<last_pool_ip> --- apiVersion : metallb.io/v1beta1 kind : L2Advertisement metadata : name : karavy-demo-advertisement namespace : metallb-system 2.3 Installazione provider CSI nel cluster di management \u00b6 Nel cluster di management installare il driver csi helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts helm install csi-driver-nfs csi-driver-nfs/csi-driver-nfs --namespace kube-system --version 4 .11.0 --set kubeletDir = \"/var/lib/k0s/kubelet\" Sempre nel cluster di management creare la storage class per il provider applicando il seguente manifest apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : nfs-csi provisioner : nfs.csi.k8s.io parameters : server : <nfs_server_ip> share : /home/nfsdata reclaimPolicy : Delete volumeBindingMode : Immediate allowVolumeExpansion : true mountOptions : - nfsvers=4.1 3. Installazione Karavy-Core \u00b6 Nel cluster di management installare i componente core per la gestione dei tenant sudo apt install -y git git clone https://github.com/karavy/karavy-demo.git kubectl apply -k karavy-demo/core Verificare che l\u2019operatore sia avviato nel namespace karavy-core . 4. Creazione del Tenant \u00b6 4.1 Namespace e CRD \u00b6 Nel cluster di management creare il namespace relativo al nuovo tenant kubectl create ns tenant-1 Applicare nel cluster di management il manifest del nuovo tenant (verificare IP e CIDR per evitare sovrapposizioni): apiVersion : tenants.karavy.io/v1 kind : K8sTenant metadata : labels : tenant-name : tenant-1 name : tenant-1 namespace : tenant-1 spec : kubeMasterDomain : cluster.local tenantApiServer : additionalArgs : - --v=10 antiAffinity : true certificateDurationHours : 8760 keycloak : enabled : false replicas : 1 tenantControllerManager : antiAffinity : true certificateDurationHours : 8760 replicas : 1 tenantDNSServer : certificateDurationHours : 8760 tenantEtcdServer : additionalArgs : - --initial-advertise-peer-urls=$(URI_SCHEME)://$(HOSTNAME).$(SERVICE_NAME).$(K8S_NAMESPACE):2380 antiAffinity : true certificateDurationHours : 8760 replicas : 3 storageClassName : nfs-csi storageSize : 10Gi tenantNetwork : cniApplication : type : calico version : v3.29.2 kubeDefaultSvc : 10.240.0.1 podCidr : 16 podNetwork : 10.120.0.0 serviceCidr : 16 serviceNetwork : 10.240.0.0 tenantKubeDNSIP : 10.240.0.10 tenantKubeDomain : tenant-1.local tenantScheduler : antiAffinity : true certificateDurationHours : 8760 replicas : 1 tenantVersions : tenantCrioVersion : v1.31.5 tenantEtcdVersion : v3.5.18 tenantKubeVersion : v1.31.5 usersCertificates : certificateDurationHours : 8760 winNodeSupport : true workerCertificates : certificateDurationHours : 8760 4.2 Connessione al Cluster Tenant \u00b6 Per prima cosa estrarre il secret relativo al nuovo tenant kubectl get secret -n tenant-1 admin-conf -o jsonpath = '{.data.tenant-1-config}' | base64 -d > /tmp/kubeconfig export KUBECONFIG = /tmp/kubeconfig Eseguendo il comando di elencazione dei pod kubectl get pods -A Il risultato visualizzato mostrer\u00e0 l'operator di Calico e il dns in Pending, in attesa cio\u00e8 di un nodo worker per eseguire il carico. NAMESPACE NAME READY STATUS RESTARTS AGE kube-system forwarder-dns-fd89bcbd5-l8wbq 0 /1 Pending 0 86m kube-system forwarder-dns-fd89bcbd5-vbnwz 0 /1 Pending 0 86m tigera-operator tigera-operator-64ff5465b7-8r8mm 0 /1 Pending 0 86m 5. Installazione Karavy-Worker \u00b6 Dal repository scaricato in precedenza, eseguire il comando per installare il gestore dei nodi worker. Al momento \u00e8 possibile creare nodi linux basati su Ubuntu 24.04. Il cluster supporta anche nodi Microsoft, la documentazione relativa verr\u00e0 pubblicata a breve. kubectl apply -k karavy-demo/workers 6. Configurazione Worker Node \u00b6 6.1 Generazione chiave ssh \u00b6 Sulla vm che ospita il cluster di management creare una coppia di chiavi ssh per root ssh-keygen Collegarsi al nodo worker e copiare la chiave pubblica appena generata nel file /root/.ssh/authorized_keys Verificare che con la chiave generata sia possibile collegarsi al nodo ssh root@<indirizzo_ip_worker_node> 6.2 Accesso SSH da parte di karavy-core \u00b6 Creare un secret nel tenant a cui il nodo worker dovr\u00e0 associarsi. Il nome del secret sar\u00e0 quello indicato nel campo sshKeySecret del cr che definisce il worker node. kubectl create secret generic sshkey -n tenant-1 --from-file = sshkey = /home/<user>/.ssh/<nome_file_chiave_privata> 6.2 Manifest Worker \u00b6 apiVersion : tenants.karavy.io/v1 kind : K8sTenantWorker metadata : name : tenant-1-worker-01 namespace : tenant-1 spec : tenantName : tenant-1 hostname : worker-01 hostIP : 172.25.58.100 sshKeySecret : sshkey operatingSystem : ubuntu2404 sshPort : 22 7. Installazione Karavy-Net \u00b6 kubectl apply -k karavy-demo/net 7.1 Definizione Rotte \u00b6 apiVersion : net.karavy.io/v1 kind : KaravyRouting metadata : labels : app.kubernetes.io/name : karavy-net app.kubernetes.io/managed-by : kustomize name : karavy-tenant-1-routing namespace : karavy-net spec : tenantName : tenant-1 mainServiceCidr : 10.96.0.0/12 mainPodCidr : 10.244.0.0/16 mainCNIType : userdefined mainClusterNodes : - mainClusterNodeType : controlplane name : kavary-main-cluster servicePriority : 10 tenantServiceCidr : 10.240.0.0/16 tenantPodCidr : 10.120.0.0/16 tenantCNIType : calico tenantClusterNodes : - ip : 192.168.3.216 name : worker-01 servicePriority : 10 sshKey : sshkey sshPort : 22","title":"Karavy Demo"},{"location":"karavy-demo/#karavy-demo-creazione-di-un-cluster-tenant","text":"","title":"Karavy Demo \u2013 Creazione di un Cluster Tenant"},{"location":"karavy-demo/#requisiti","text":"Per questa demo sono necessarie due macchine (fisiche o virtuali): Prima VM : Cluster di management CPU: 2 RAM: 4GB Disk: 40GB Seconda VM : Worker node del cluster tenant CPU: 2 RAM: 4GB Disk: 40GB Terza VM : (Opzionale) Installazione del server NFS per i volumi etcd del cluster tenant . Nel caso in cui si disponga gi\u00e0 di storage condiviso disponibile per il cluster di management, si pu\u00f2 utilizzare quello. CPU: 1 RAM: 2GB Disk: 40GB Le macchine devono essere sulla stessa rete (o comunque raggiungibili tra loro senza limitazione di porte o protocolli). In questa demo sono state usate VM su KVM.","title":"Requisiti"},{"location":"karavy-demo/#1-installazione-del-cluster-di-management","text":"","title":"1. Installazione del Cluster di Management"},{"location":"karavy-demo/#11-sistema-operativo","text":"Installare Ubuntu 24.04 Server e assegnare un IP statico ad entrambe le macchine. Durante l'installazione attivare anche il server ssh","title":"1.1 Sistema Operativo"},{"location":"karavy-demo/#12-installazione-di-k0s","text":"Collegarsi alla vm che ospiter\u00e0 il cluster di management e installare k0s curl --proto '=https' --tlsv1.2 -sSf https://get.k0s.sh | sudo sh sudo k0s install controller --single sudo k0s start","title":"1.2 Installazione di k0s"},{"location":"karavy-demo/#13-installare-e-configurare-kubectl-per-laccesso-al-cluster-di-management","text":"sudo -i snap install kubectl --classic mkdir /home/<user>/.kube k0s kubeconfig admin > /home/<user>/.kube/config chown -R <user>:<group> /home/<user>/.kube exit","title":"1.3 Installare e configurare kubectl per l'accesso al cluster di management"},{"location":"karavy-demo/#14-installazione-di-helm","text":"curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null sudo apt-get install apt-transport-https --yes echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm","title":"1.4 Installazione di Helm"},{"location":"karavy-demo/#15-installazione-di-storage-condiviso-per-i-volumi-di-etcd","text":"","title":"1.5 Installazione di storage condiviso per i volumi di etcd"},{"location":"karavy-demo/#151-installazione-e-configurazione-server-nfs","text":"Installare il server NFS e creare la directory che conterr\u00e0 i volumi kubernetes sudo apt install nfs-server sudo mkdir /home/nfsdata Creare la condivisione NFS sudo -i echo '/home/nfsdata <cluster_network>(no_root_squash,rw,sync,no_subtree_check)' >> /etc/exports exportfs -va","title":"1.5.1 Installazione e configurazione server NFS"},{"location":"karavy-demo/#2-installazione-componenti-del-cluster","text":"","title":"2. Installazione Componenti del Cluster"},{"location":"karavy-demo/#21-cert-manager","text":"helm repo add jetstack https://charts.jetstack.io --force-update helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --version v1.17.2 --set crds.enabled = true","title":"2.1 Cert-Manager"},{"location":"karavy-demo/#22-metallb","text":"kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.14.9/config/manifests/metallb-native.yaml","title":"2.2 MetalLB"},{"location":"karavy-demo/#221-configurazione-ipaddresspool-e-l2advertisement","text":"Configurare Metallb. Nei manifest seguenti si utilizza la modalit\u00e0 layer2 apiVersion : metallb.io/v1beta1 kind : IPAddressPool metadata : name : karavy-demo-pool namespace : metallb-system spec : addresses : - <first_pool_ip>-<last_pool_ip> --- apiVersion : metallb.io/v1beta1 kind : L2Advertisement metadata : name : karavy-demo-advertisement namespace : metallb-system","title":"2.2.1 Configurazione IPAddressPool e L2Advertisement"},{"location":"karavy-demo/#23-installazione-provider-csi-nel-cluster-di-management","text":"Nel cluster di management installare il driver csi helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts helm install csi-driver-nfs csi-driver-nfs/csi-driver-nfs --namespace kube-system --version 4 .11.0 --set kubeletDir = \"/var/lib/k0s/kubelet\" Sempre nel cluster di management creare la storage class per il provider applicando il seguente manifest apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : nfs-csi provisioner : nfs.csi.k8s.io parameters : server : <nfs_server_ip> share : /home/nfsdata reclaimPolicy : Delete volumeBindingMode : Immediate allowVolumeExpansion : true mountOptions : - nfsvers=4.1","title":"2.3 Installazione provider CSI nel cluster di management"},{"location":"karavy-demo/#3-installazione-karavy-core","text":"Nel cluster di management installare i componente core per la gestione dei tenant sudo apt install -y git git clone https://github.com/karavy/karavy-demo.git kubectl apply -k karavy-demo/core Verificare che l\u2019operatore sia avviato nel namespace karavy-core .","title":"3. Installazione Karavy-Core"},{"location":"karavy-demo/#4-creazione-del-tenant","text":"","title":"4. Creazione del Tenant"},{"location":"karavy-demo/#41-namespace-e-crd","text":"Nel cluster di management creare il namespace relativo al nuovo tenant kubectl create ns tenant-1 Applicare nel cluster di management il manifest del nuovo tenant (verificare IP e CIDR per evitare sovrapposizioni): apiVersion : tenants.karavy.io/v1 kind : K8sTenant metadata : labels : tenant-name : tenant-1 name : tenant-1 namespace : tenant-1 spec : kubeMasterDomain : cluster.local tenantApiServer : additionalArgs : - --v=10 antiAffinity : true certificateDurationHours : 8760 keycloak : enabled : false replicas : 1 tenantControllerManager : antiAffinity : true certificateDurationHours : 8760 replicas : 1 tenantDNSServer : certificateDurationHours : 8760 tenantEtcdServer : additionalArgs : - --initial-advertise-peer-urls=$(URI_SCHEME)://$(HOSTNAME).$(SERVICE_NAME).$(K8S_NAMESPACE):2380 antiAffinity : true certificateDurationHours : 8760 replicas : 3 storageClassName : nfs-csi storageSize : 10Gi tenantNetwork : cniApplication : type : calico version : v3.29.2 kubeDefaultSvc : 10.240.0.1 podCidr : 16 podNetwork : 10.120.0.0 serviceCidr : 16 serviceNetwork : 10.240.0.0 tenantKubeDNSIP : 10.240.0.10 tenantKubeDomain : tenant-1.local tenantScheduler : antiAffinity : true certificateDurationHours : 8760 replicas : 1 tenantVersions : tenantCrioVersion : v1.31.5 tenantEtcdVersion : v3.5.18 tenantKubeVersion : v1.31.5 usersCertificates : certificateDurationHours : 8760 winNodeSupport : true workerCertificates : certificateDurationHours : 8760","title":"4.1 Namespace e CRD"},{"location":"karavy-demo/#42-connessione-al-cluster-tenant","text":"Per prima cosa estrarre il secret relativo al nuovo tenant kubectl get secret -n tenant-1 admin-conf -o jsonpath = '{.data.tenant-1-config}' | base64 -d > /tmp/kubeconfig export KUBECONFIG = /tmp/kubeconfig Eseguendo il comando di elencazione dei pod kubectl get pods -A Il risultato visualizzato mostrer\u00e0 l'operator di Calico e il dns in Pending, in attesa cio\u00e8 di un nodo worker per eseguire il carico. NAMESPACE NAME READY STATUS RESTARTS AGE kube-system forwarder-dns-fd89bcbd5-l8wbq 0 /1 Pending 0 86m kube-system forwarder-dns-fd89bcbd5-vbnwz 0 /1 Pending 0 86m tigera-operator tigera-operator-64ff5465b7-8r8mm 0 /1 Pending 0 86m","title":"4.2 Connessione al Cluster Tenant"},{"location":"karavy-demo/#5-installazione-karavy-worker","text":"Dal repository scaricato in precedenza, eseguire il comando per installare il gestore dei nodi worker. Al momento \u00e8 possibile creare nodi linux basati su Ubuntu 24.04. Il cluster supporta anche nodi Microsoft, la documentazione relativa verr\u00e0 pubblicata a breve. kubectl apply -k karavy-demo/workers","title":"5. Installazione Karavy-Worker"},{"location":"karavy-demo/#6-configurazione-worker-node","text":"","title":"6. Configurazione Worker Node"},{"location":"karavy-demo/#61-generazione-chiave-ssh","text":"Sulla vm che ospita il cluster di management creare una coppia di chiavi ssh per root ssh-keygen Collegarsi al nodo worker e copiare la chiave pubblica appena generata nel file /root/.ssh/authorized_keys Verificare che con la chiave generata sia possibile collegarsi al nodo ssh root@<indirizzo_ip_worker_node>","title":"6.1 Generazione chiave ssh"},{"location":"karavy-demo/#62-accesso-ssh-da-parte-di-karavy-core","text":"Creare un secret nel tenant a cui il nodo worker dovr\u00e0 associarsi. Il nome del secret sar\u00e0 quello indicato nel campo sshKeySecret del cr che definisce il worker node. kubectl create secret generic sshkey -n tenant-1 --from-file = sshkey = /home/<user>/.ssh/<nome_file_chiave_privata>","title":"6.2 Accesso SSH da parte di karavy-core"},{"location":"karavy-demo/#62-manifest-worker","text":"apiVersion : tenants.karavy.io/v1 kind : K8sTenantWorker metadata : name : tenant-1-worker-01 namespace : tenant-1 spec : tenantName : tenant-1 hostname : worker-01 hostIP : 172.25.58.100 sshKeySecret : sshkey operatingSystem : ubuntu2404 sshPort : 22","title":"6.2 Manifest Worker"},{"location":"karavy-demo/#7-installazione-karavy-net","text":"kubectl apply -k karavy-demo/net","title":"7. Installazione Karavy-Net"},{"location":"karavy-demo/#71-definizione-rotte","text":"apiVersion : net.karavy.io/v1 kind : KaravyRouting metadata : labels : app.kubernetes.io/name : karavy-net app.kubernetes.io/managed-by : kustomize name : karavy-tenant-1-routing namespace : karavy-net spec : tenantName : tenant-1 mainServiceCidr : 10.96.0.0/12 mainPodCidr : 10.244.0.0/16 mainCNIType : userdefined mainClusterNodes : - mainClusterNodeType : controlplane name : kavary-main-cluster servicePriority : 10 tenantServiceCidr : 10.240.0.0/16 tenantPodCidr : 10.120.0.0/16 tenantCNIType : calico tenantClusterNodes : - ip : 192.168.3.216 name : worker-01 servicePriority : 10 sshKey : sshkey sshPort : 22","title":"7.1 Definizione Rotte"}]}